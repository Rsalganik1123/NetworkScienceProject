{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_categorical\n",
    "import torch\n",
    "import pandas as pd \n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score, ndcg_score\n",
    "import tqdm \n",
    "import scipy.sparse as ssp\n",
    "\n",
    "# import layers\n",
    "# import sampler as sampler_module\n",
    "# import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _series_to_tensor(series):\n",
    "    if is_categorical(series):\n",
    "        return torch.LongTensor(series.cat.codes.values.astype('int64'))\n",
    "    else:       # numeric\n",
    "        return torch.FloatTensor(series.values)\n",
    "\n",
    "class PandasGraphBuilder(object):\n",
    "    def __init__(self):\n",
    "        self.entity_tables = {}\n",
    "        self.relation_tables = {}\n",
    "\n",
    "        self.entity_pk_to_name = {}     # mapping from primary key name to entity name\n",
    "        self.entity_pk = {}             # mapping from entity name to primary key\n",
    "        self.entity_key_map = {}        # mapping from entity names to primary key values\n",
    "        self.num_nodes_per_type = {}\n",
    "        self.edges_per_relation = {}\n",
    "        self.relation_name_to_etype = {}\n",
    "        self.relation_src_key = {}      # mapping from relation name to source key\n",
    "        self.relation_dst_key = {}      # mapping from relation name to destination key\n",
    "\n",
    "    def add_entities(self, entity_table, primary_key, name):\n",
    "        entities = entity_table[primary_key].astype('category')\n",
    "        if not (entities.value_counts() == 1).all():\n",
    "            raise ValueError('Different entity with the same primary key detected.')\n",
    "        # preserve the category order in the original entity table\n",
    "        entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n",
    "\n",
    "        self.entity_pk_to_name[primary_key] = name\n",
    "        self.entity_pk[name] = primary_key\n",
    "        self.num_nodes_per_type[name] = entity_table.shape[0]\n",
    "        self.entity_key_map[name] = entities\n",
    "        self.entity_tables[name] = entity_table\n",
    "\n",
    "    def add_binary_relations(self, relation_table, source_key, destination_key, name):\n",
    "        src = relation_table[source_key].astype('category')\n",
    "        src = src.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[source_key]].cat.categories)\n",
    "        dst = relation_table[destination_key].astype('category')\n",
    "        dst = dst.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[destination_key]].cat.categories)\n",
    "        if src.isnull().any():\n",
    "            raise ValueError(\n",
    "                'Some source entities in relation %s do not exist in entity %s.' %\n",
    "                (name, source_key))\n",
    "        if dst.isnull().any():\n",
    "            raise ValueError(\n",
    "                'Some destination entities in relation %s do not exist in entity %s.' %\n",
    "                (name, destination_key))\n",
    "\n",
    "        srctype = self.entity_pk_to_name[source_key]\n",
    "        dsttype = self.entity_pk_to_name[destination_key]\n",
    "        etype = (srctype, name, dsttype)\n",
    "        self.relation_name_to_etype[name] = etype\n",
    "        self.edges_per_relation[etype] = (src.cat.codes.values.astype('int64'), dst.cat.codes.values.astype('int64'))\n",
    "        self.relation_tables[name] = relation_table\n",
    "        self.relation_src_key[name] = source_key\n",
    "        self.relation_dst_key[name] = destination_key\n",
    "\n",
    "    def build(self):\n",
    "        # Create heterograph\n",
    "        graph = dgl.heterograph(self.edges_per_relation, self.num_nodes_per_type)\n",
    "        return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_pid(df, group_by_val, train_size=.8, val_size = .1, test_size=.1):\n",
    "    print(\"***Splitting by Playlist***\")\n",
    "    train_pids, all_test_pids = train_test_split(df[group_by_val].unique(), test_size=test_size+val_size, random_state=1)\n",
    "    all_test = df[df.pid.isin(all_test_pids)]\n",
    "    val_pids, test_pids = train_test_split(all_test[group_by_val].unique(), test_size=val_size, random_state=1)\n",
    "    train = df[df.pid.isin(train_pids)]\n",
    "    val = df[df.pid.isin(val_pids)]\n",
    "    test = df[df.pid.isin(test_pids)]\n",
    "    \n",
    "    print(\"***Current Set has {} pids in train, {} pids in val, {} pids in test\".format(len(train_pids), len(val_pids), len(test_pids)))\n",
    "    return list(train.index), list(val.index), list(test.index)\n",
    "\n",
    "def build_subgraph_graph(g, train_indices, utype, itype, etype, etype_rev):\n",
    "    train_g = g.edge_subgraph(\n",
    "        {etype: train_indices, etype_rev: train_indices},\n",
    "        ) \n",
    "    try: \n",
    "        for ntype in g.ntypes:\n",
    "            for col, data in g.nodes[ntype].data.items(): \n",
    "                train_g.nodes[ntype].data[col] = data\n",
    "        for etype in g.etypes:\n",
    "            for col, data in g.edges[etype].data.items():\n",
    "                train_g.edges[etype].data[col] = data[train_g.edges[etype].data[dgl.EID]]\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    return train_g\n",
    "\n",
    "def build_val_test_matrix(g, val_indices, test_indices, utype, itype, etype):\n",
    "    n_users = g.number_of_nodes(utype)\n",
    "    n_items = g.number_of_nodes(itype)\n",
    "    val_src, val_dst = g.find_edges(val_indices, etype=etype)\n",
    "    test_src, test_dst = g.find_edges(test_indices, etype=etype)\n",
    "    val_src = val_src.numpy()\n",
    "    val_dst = val_dst.numpy()\n",
    "    test_src = test_src.numpy()\n",
    "    test_dst = test_dst.numpy()\n",
    "    val_matrix = ssp.coo_matrix((np.ones_like(val_src), (val_src, val_dst)), (n_users, n_items))\n",
    "    test_matrix = ssp.coo_matrix((np.ones_like(test_src), (test_src, test_dst)), (n_users, n_items))\n",
    "\n",
    "    return val_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemToItemBatchSampler(IterableDataset):\n",
    "    def __init__(self, g, user_type, item_type, batch_size):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            heads = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n",
    "            tails = dgl.sampling.random_walk(\n",
    "                self.g,\n",
    "                heads,\n",
    "                metapath=[self.item_to_user_etype, self.user_to_item_etype])[0][:, 2]\n",
    "            neg_tails = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n",
    "\n",
    "            mask = (tails != -1)\n",
    "            yield heads[mask], tails[mask], neg_tails[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhood Sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_and_copy(frontier, seeds):\n",
    "    block = dgl.to_block(frontier, seeds)\n",
    "    for col, data in frontier.edata.items():\n",
    "        if col == dgl.EID:\n",
    "            continue\n",
    "        block.edata[col] = data[block.edata[dgl.EID]]\n",
    "    return block\n",
    "\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,\n",
    "                 num_random_walks, num_neighbors, num_layers):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.samplers = [\n",
    "            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,\n",
    "                                        random_walk_restart_prob, num_random_walks, num_neighbors)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n",
    "        blocks = []\n",
    "        for sampler in self.samplers:\n",
    "            frontier = sampler(seeds)\n",
    "            if heads is not None:\n",
    "                eids = frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), return_uv=True)[2]\n",
    "                if len(eids) > 0:\n",
    "                    old_frontier = frontier\n",
    "                    frontier = dgl.remove_edges(old_frontier, eids)\n",
    "                    # print(old_frontier)\n",
    "                    # print(frontier)\n",
    "                    # print(frontier.edata['weights'])\n",
    "                    # frontier.edata['weights'] = old_frontier.edata['weights'][frontier.edata[dgl.EID]]\n",
    "            block = compact_and_copy(frontier, seeds)\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block)\n",
    "        return blocks\n",
    "\n",
    "    def sample_from_item_pairs(self, heads, tails, neg_tails):\n",
    "        # Create a graph with positive connections only and another graph with negative\n",
    "        # connections only.\n",
    "        pos_graph = dgl.graph(\n",
    "            (heads, tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.item_type))\n",
    "        neg_graph = dgl.graph(\n",
    "            (heads, neg_tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.item_type))\n",
    "        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "\n",
    "        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "def assign_features_to_blocks(blocks, g, ntype='track'):\n",
    "    \n",
    "    data = blocks[0].srcdata\n",
    "    \n",
    "    for col in g.nodes[ntype].data.keys():\n",
    "        if  col == dgl.NID:\n",
    "            continue\n",
    "        induced_nodes = data[dgl.NID]\n",
    "        data[col] = g.nodes[ntype].data[col][induced_nodes]\n",
    "\n",
    "    \n",
    "    data = blocks[-1].dstdata\n",
    "    for col in g.nodes[ntype].data.keys():\n",
    "        if  col == dgl.NID:\n",
    "            continue\n",
    "        induced_nodes = data[dgl.NID]\n",
    "        data[col] = g.nodes[ntype].data[col][induced_nodes]\n",
    "\n",
    "class PinSAGECollator(object):\n",
    "    def __init__(self, sampler, g_train, g_test, ntype):\n",
    "        self.sampler = sampler\n",
    "        self.ntype = ntype\n",
    "        self.g_train = g_train\n",
    "        self.g_test = g_test\n",
    "\n",
    "    def collate_train(self, batches):\n",
    "        heads, tails, neg_tails = batches[0]\n",
    "        # Construct multilayer neighborhood via PinSAGE...\n",
    "        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "        assign_features_to_blocks(blocks, self.g_train, self.ntype)\n",
    "\n",
    "        return pos_graph, neg_graph, blocks\n",
    "    \n",
    "    def collate_test(self, samples):\n",
    "        batch = torch.LongTensor(samples) \n",
    "        blocks = self.sampler.sample_blocks(batch)\n",
    "        assign_features_to_blocks(blocks, self.g_test, self.ntype)\n",
    "        return blocks        \n",
    "\n",
    "# class PinSAGECollator(object): #OG VERSION\n",
    "#     def __init__(self, sampler, g, ntype):\n",
    "#         self.sampler = sampler\n",
    "#         self.ntype = ntype\n",
    "#         self.g_ = g\n",
    "\n",
    "#     def collate_train(self, batches):\n",
    "#         heads, tails, neg_tails = batches[0]\n",
    "#         # Construct multilayer neighborhood via PinSAGE...\n",
    "#         pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "#         assign_features_to_blocks(blocks, self.g, self.ntype)\n",
    "\n",
    "#         return pos_graph, neg_graph, blocks\n",
    "    \n",
    "#     def collate_test(self, samples):\n",
    "#         batch = torch.LongTensor(samples) \n",
    "#         blocks = self.sampler.sample_blocks(batch)\n",
    "#         assign_features_to_blocks(blocks, self.g, self.ntype)\n",
    "#         return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_grad(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def _init_input_modules(g, ntype):\n",
    "    module_dict = nn.ModuleDict()\n",
    "    \n",
    "    tracks_data = g.nodes[ntype].data\n",
    "    \n",
    "    module_dict['track_id'] = nn.Embedding(tracks_data['id'].max()+1, 128)\n",
    "    \n",
    "    for m in ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']: \n",
    "        module_dict[m] = nn.Embedding(tracks_data[m].max() + 1, 16)\n",
    "\n",
    "    return module_dict\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects each input feature of the graph linearly and sums them up\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, full_graph, ntype):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ntype = ntype\n",
    "        self.fc = nn.Linear(164, 128)\n",
    "        #self.fc = nn.Linear(2212, 128)\n",
    "        self.inputs = _init_input_modules(full_graph, ntype)\n",
    "\n",
    "    def forward(self, ndata):\n",
    "        \n",
    "        # get music feature\n",
    "        music_features = []\n",
    "        for c in ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']:\n",
    "\n",
    "            module = self.inputs[c]\n",
    "            music_features.append(module(ndata[c]))\n",
    "        music_features = torch.cat(music_features, dim=1)\n",
    "        \n",
    "        \n",
    "#         # id embedding \n",
    "#         id_embedding = self.inputs['track_id'](ndata['id'])\n",
    "        \n",
    "#         # album feature \n",
    "#         img_emb = ndata['album_img_emb']\n",
    "        \n",
    "        # genre \n",
    "        genre = ndata['genre']\n",
    "        \n",
    "        # concatenate \n",
    "        #feature = torch.cat([music_features, genre, ndata['album_img_emb']], dim=1)\n",
    "        feature = torch.cat([music_features, genre], dim=1)\n",
    "\n",
    "        projection = self.fc(feature) #+ id_embedding\n",
    "        \n",
    "        return projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageNet Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class WeightedSAGEConv(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, act=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = act\n",
    "        self.Q = nn.Linear(input_dims, hidden_dims)\n",
    "        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n",
    "        nn.init.constant_(self.Q.bias, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "    def forward(self, g, h, weights):\n",
    "        \"\"\"\n",
    "        g : graph\n",
    "        h : node features\n",
    "        weights : scalar edge weights\n",
    "        \"\"\"\n",
    "        h_src, h_dst = h\n",
    "        with g.local_scope():\n",
    "            g.srcdata['n'] = self.act(self.Q(self.dropout(h_src)))\n",
    "            g.edata['w'] = weights.float()\n",
    "            g.update_all(fn.u_mul_e('n', 'w', 'm'), fn.sum('m', 'n'))\n",
    "            g.update_all(fn.copy_e('w', 'm'), fn.sum('m', 'ws'))\n",
    "            n = g.dstdata['n']\n",
    "            ws = g.dstdata['ws'].unsqueeze(1).clamp(min=1)\n",
    "            z = self.act(self.W(self.dropout(torch.cat([n / ws, h_dst], 1))))\n",
    "            z_norm = z.norm(2, 1, keepdim=True)\n",
    "            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n",
    "            z = z / z_norm\n",
    "            return z\n",
    "\n",
    "\n",
    "class SAGENet(nn.Module):\n",
    "    def __init__(self, hidden_dims, n_layers):\n",
    "        \"\"\"\n",
    "        g : DGLHeteroGraph\n",
    "            The user-item interaction graph.\n",
    "            This is only for finding the range of categorical variables.\n",
    "        item_textsets : torchtext.data.Dataset\n",
    "            The textual features of each item node.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))\n",
    "\n",
    "    def forward(self, blocks, h):\n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            h_dst = h[:block.number_of_nodes('DST/' + block.ntypes[0])]\n",
    "            h = layer(block, (h, h_dst), block.edata['weights'])\n",
    "        return h\n",
    "\n",
    "class ItemToItemScorer(nn.Module):\n",
    "    def __init__(self, full_graph, ntype):\n",
    "        super().__init__()\n",
    "\n",
    "        n_nodes = full_graph.number_of_nodes(ntype)\n",
    "        self.bias = nn.Parameter(torch.zeros(n_nodes))\n",
    "\n",
    "    def _add_bias(self, edges):\n",
    "        bias_src = self.bias[edges.src[dgl.NID]]\n",
    "        bias_dst = self.bias[edges.dst[dgl.NID]]\n",
    "        return {'s': edges.data['s'] + bias_src + bias_dst}\n",
    "\n",
    "    def forward(self, item_item_graph, h):\n",
    "        \"\"\"\n",
    "        item_item_graph : graph consists of edges connecting the pairs\n",
    "        h : hidden state of every node\n",
    "        \"\"\"\n",
    "        with item_item_graph.local_scope():\n",
    "            item_item_graph.ndata['h'] = h\n",
    "            item_item_graph.apply_edges(fn.u_dot_v('h', 'h', 's'))\n",
    "            item_item_graph.edata['s'] = item_item_graph.edata['s'].flatten()\n",
    "            item_item_graph.apply_edges(self._add_bias)\n",
    "\n",
    "            pair_score = item_item_graph.edata['s']\n",
    "        return pair_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).detach().cpu().numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "class PinSAGEModel(nn.Module):\n",
    "    def __init__(self, full_graph, ntype, hidden_dims, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = LinearProjector(full_graph, ntype)\n",
    "        self.sage = SAGENet(hidden_dims, n_layers)\n",
    "        self.scorer = ItemToItemScorer(full_graph, ntype)\n",
    "\n",
    "    def forward(self, pos_graph, neg_graph, blocks):\n",
    "        h_item = self.get_repr(blocks)\n",
    "        pos_score = self.scorer(pos_graph, h_item)\n",
    "        neg_score = self.scorer(neg_graph, h_item)\n",
    "        \n",
    "        #return h_item, pos_score, neg_score\n",
    "        auc = compute_auc(pos_score, neg_score)\n",
    "        return (neg_score - pos_score + 1).clamp(min=0), auc \n",
    "\n",
    "\n",
    "    def get_repr(self, blocks):\n",
    "        h_item = self.proj(blocks[0].srcdata)\n",
    "        h_item_dst = self.proj(blocks[-1].dstdata)\n",
    "        return h_item_dst + self.sage(blocks, h_item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_topk(g, k, weight, nodes=None, edge_dir='in', ascending=False,\n",
    "                copy_ndata=True, copy_edata=True):\n",
    "    \n",
    "    # Rectify nodes to a dictionary\n",
    "    if nodes is None:\n",
    "        nodes = {\n",
    "            ntype: F.astype(F.arange(0, g.number_of_nodes(ntype)), g.idtype)\n",
    "            for ntype in g.ntypes\n",
    "        }\n",
    "    elif not isinstance(nodes, dict):\n",
    "        if len(g.ntypes) > 1:\n",
    "            raise DGLError(\"Must specify node type when the graph is not homogeneous.\")\n",
    "        nodes = {g.ntypes[0] : nodes}\n",
    "    assert g.device == F.cpu(), \"Graph must be on CPU.\"\n",
    "\n",
    "    # Parse nodes into a list of NDArrays.\n",
    "    nodes = utils.prepare_tensor_dict(g, nodes, 'nodes')\n",
    "    nodes_all_types = []\n",
    "    for ntype in g.ntypes:\n",
    "        if ntype in nodes:\n",
    "            nodes_all_types.append(F.to_dgl_nd(nodes[ntype]))\n",
    "        else:\n",
    "            nodes_all_types.append(nd.array([], ctx=nd.cpu()))\n",
    "\n",
    "    if not isinstance(k, dict):\n",
    "        k_array = [int(k)] * len(g.etypes)\n",
    "    else:\n",
    "        if len(k) != len(g.etypes):\n",
    "            raise DGLError('K value must be specified for each edge type '\n",
    "                           'if a dict is provided.')\n",
    "        k_array = [None] * len(g.etypes)\n",
    "        for etype, value in k.items():\n",
    "            k_array[g.get_etype_id(etype)] = value\n",
    "    k_array = F.to_dgl_nd(F.tensor(k_array, dtype=F.int64))\n",
    "\n",
    "    weight_arrays = []\n",
    "    for etype in g.canonical_etypes:\n",
    "        if weight in g.edges[etype].data:\n",
    "            weight_arrays.append(F.to_dgl_nd(g.edges[etype].data[weight]))\n",
    "        else:\n",
    "            raise DGLError('Edge weights \"{}\" do not exist for relation graph \"{}\".'.format(\n",
    "                weight, etype))\n",
    "\n",
    "    subgidx = _CAPI_DGLSampleNeighborsTopk(\n",
    "        g._graph, nodes_all_types, k_array, edge_dir, weight_arrays, bool(ascending))\n",
    "    induced_edges = subgidx.induced_edges\n",
    "    ret = DGLHeteroGraph(subgidx.graph, g.ntypes, g.etypes)\n",
    "\n",
    "    # handle features\n",
    "    if copy_ndata:\n",
    "        node_frames = utils.extract_node_subframes(g, None)\n",
    "        utils.set_new_frames(ret, node_frames=node_frames)\n",
    "\n",
    "    if copy_edata:\n",
    "        edge_frames = utils.extract_edge_subframes(g, induced_edges)\n",
    "        utils.set_new_frames(ret, edge_frames=edge_frames)\n",
    "    return ret\n",
    "\n",
    "def prec(recommendations, ground_truth):\n",
    "    n_users, n_items = ground_truth.shape\n",
    "    K = recommendations.shape[1]\n",
    "    user_idx = np.repeat(np.arange(n_users), K)\n",
    "    item_idx = recommendations.flatten()\n",
    "    relevance = ground_truth[user_idx, item_idx].reshape((n_users, K))\n",
    "    hit = relevance.any(axis=1).mean()\n",
    "    return hit\n",
    "\n",
    "class LatestNNRecommender(object):\n",
    "    def __init__(self, user_ntype, item_ntype, user_to_item_etype, batch_size):\n",
    "        self.user_ntype = user_ntype\n",
    "        self.item_ntype = item_ntype\n",
    "        self.user_to_item_etype = user_to_item_etype\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def recommend(self, full_graph, K, h_user, h_item):\n",
    "        \"\"\"\n",
    "        Return a (n_user, K) matrix of recommended items for each user\n",
    "        \"\"\"\n",
    "        graph_slice = full_graph.edge_type_subgraph([self.user_to_item_etype])\n",
    "        n_users = full_graph.number_of_nodes(self.user_ntype)\n",
    "        latest_interactions = select_topk(graph_slice, 1, 'weights', edge_dir='out')\n",
    "        user, latest_items = latest_interactions.all_edges(form='uv', order='srcdst')\n",
    "        # each user should have at least one \"latest\" interaction\n",
    "        assert torch.equal(user, torch.arange(n_users))\n",
    "\n",
    "        recommended_batches = []\n",
    "        user_batches = torch.arange(n_users).split(self.batch_size)\n",
    "        for user_batch in user_batches:\n",
    "            latest_item_batch = latest_items[user_batch].to(device=h_item.device)\n",
    "            dist = h_item[latest_item_batch] @ h_item.t()\n",
    "            # exclude items that are already interacted\n",
    "            for i, u in enumerate(user_batch.tolist()):\n",
    "                interacted_items = full_graph.successors(u, etype=self.user_to_item_etype)\n",
    "                dist[i, interacted_items] = -np.inf\n",
    "            recommended_batches.append(dist.topk(K, 1)[1])\n",
    "\n",
    "        recommendations = torch.cat(recommended_batches, 0)\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def evaluate_nn(dataset, h_item, k, batch_size):\n",
    "    g = dataset['train-graph']\n",
    "    val_matrix = dataset['val-matrix'].tocsr()\n",
    "    test_matrix = dataset['test-matrix'].tocsr()\n",
    "    item_texts = dataset['item-texts']\n",
    "    user_ntype = dataset['user-type']\n",
    "    item_ntype = dataset['item-type']\n",
    "    user_to_item_etype = dataset['user-to-item-type']\n",
    "\n",
    "    rec_engine = LatestNNRecommender(\n",
    "        user_ntype, item_ntype, user_to_item_etype, batch_size)\n",
    "\n",
    "    recommendations = rec_engine.recommend(g, k, 'contains', h_item).cpu().numpy() #was None, now 'contains'\n",
    "    return prec(recommendations, val_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading (If needed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/home/mila/r/rebecca.salganik/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_music_all_data = pickle.load(open(directory+'ns_music_all_data_ming.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playlists = ns_music_all_data['df_playlist']\n",
    "df_playlists_info = ns_music_all_data['df_playlist_info']\n",
    "df_tracks = ns_music_all_data['df_track']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playlists_info = df_playlists_info.sort_values('pid').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Building (If needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = PandasGraphBuilder()\n",
    "graph_builder.add_entities(df_tracks, 'tid', 'track')\n",
    "graph_builder.add_entities(df_playlists_info, 'pid', 'playlist')\n",
    "graph_builder.add_binary_relations(df_playlists, 'pid', 'tid', 'contains')\n",
    "graph_builder.add_binary_relations(df_playlists, 'tid', 'pid', 'contained_by')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graph_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']:\n",
    "    \n",
    "    g.nodes['track'].data[key] = torch.LongTensor(df_tracks[key].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes['track'].data['genre'] = torch.tensor(np.asarray(list(df_tracks['genre'].values))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.nodes['track'].data['album_img_emb'] = torch.tensor(np.asarray(list(df_tracks['album_img_emb'].values)))\n",
    "# g.nodes['track'].data['album_text_emb'] = torch.tensor(np.asarray(list(df_tracks['album_text_emb'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes['playlist'].data['id'] = torch.arange(g.number_of_nodes('playlist'))\n",
    "g.nodes['track'].data['id'] = torch.arange(g.number_of_nodes('track'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_nodes('playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Splitting by Playlist***\n",
      "***Current Set has 800000 pids in train, 180000 pids in val, 20000 pids in test\n"
     ]
    }
   ],
   "source": [
    "train_indices, val_indices, test_indices = split_by_pid(df = df_playlists, group_by_val = 'pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect number of features to match number of nodes (len(u)). Got 1000000 and 800000 instead.\n"
     ]
    }
   ],
   "source": [
    "train_g = build_subgraph_graph(g, train_indices, 'track', 'playlist', 'contains', 'contained_by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n"
     ]
    }
   ],
   "source": [
    "print(train_g.number_of_nodes('playlist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect number of features to match number of nodes (len(u)). Got 1000000 and 180000 instead.\n"
     ]
    }
   ],
   "source": [
    "val_g = build_subgraph_graph(g, val_indices, 'track', 'playlist', 'contains', 'contained_by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "print(val_g.number_of_nodes('playlist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_nodes('playlist'), train_g.number_of_nodes('playlist'),val_g.number_of_nodes('playlist') \n",
    "train_g.number_of_nodes('playlist') + val_g.number_of_nodes('playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_matrix, test_matrix = build_val_test_matrix(g, val_indices, test_indices, 'playlist', 'track', 'contains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "        'full-graph': g,\n",
    "        'train-graph': train_g,\n",
    "        'val-graph': val_g, \n",
    "        'val-matrix': val_matrix,\n",
    "        'test-matrix': test_matrix,\n",
    "        'item-texts': None,\n",
    "        'item-images': None,\n",
    "        'user-type': 'playlist',\n",
    "        'item-type': 'track',\n",
    "        'user-to-item-type': 'contained_by',\n",
    "        'item-to-user-type': 'contains'}\n",
    "with open(directory+\"dataset_without_im.pkl\", 'wb') as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/home/mila/r/rebecca.salganik/'\n",
    "with open(directory + 'dataset_without_im.pkl', 'rb') as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset['full-graph']\n",
    "train_g = dataset['train-graph']\n",
    "val_g = dataset['val-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes['playlist'].data['id'] = torch.arange(g.number_of_nodes('playlist'))\n",
    "g.nodes['track'].data['id'] = torch.arange(g.number_of_nodes('track'))\n",
    "\n",
    "# g_test.nodes['playlist'].data['id'] = torch.arange(g_test.number_of_nodes('playlist'))\n",
    "# g_test.nodes['track'].data['id'] = torch.arange(g_test.number_of_nodes('track'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add dummy weight to edges for evaluation step\n",
    "g.edges['contains'].data['weights'] = torch.ones(g.number_of_edges('contains'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 800000, 180000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(g.ndata['id']['track']), len(train_g.ndata['id']['track']), len(val_g.ndata['id']['track'])\n",
    "g.number_of_nodes('playlist'), train_g.number_of_nodes('playlist'), val_g.number_of_nodes('playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     1,      2,      3,  ..., 999996, 999997, 999998]),\n",
       " tensor([ 970724,  970725,  970726,  ..., 1150721, 1150722, 1150723]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_g.ndata['id']['playlist'].unique(), val_g.ndata['id']['playlist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_track_ids = val_g.ndata['id']['track'][-1] + 1\n",
    "# val_playlist_ids = val_g.ndata['id']['playlist'][-1] + 1\n",
    "# val_g.nodes['track'].data['id'] = torch.arange(val_track_ids, val_track_ids+val_g.number_of_nodes('track') )\n",
    "# val_g.nodes['playlist'].data['id'] = torch.arange(val_playlist_ids, val_playlist_ids+val_g.number_of_nodes('playlist') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler\n",
    "batch_sampler = ItemToItemBatchSampler(g, 'playlist', 'track', 32) #was g\n",
    "neighbor_sampler = NeighborSampler(g, 'playlist', 'track', \n",
    "                                   random_walk_length=2, random_walk_restart_prob=0.5, num_random_walks=10, num_neighbors=3, num_layers=2)\n",
    "collator = PinSAGECollator(neighbor_sampler, train_g, val_g, 'track')\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    batch_sampler,\n",
    "    collate_fn=collator.collate_train,\n",
    "    num_workers=4) \n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "#     torch.arange(g.number_of_nodes(item_ntype)),\n",
    "#     torch.randint(1, g.number_of_nodes(item_ntype), (4000,)),\n",
    "    torch.arange(val_track_ids, val_track_ids+val_g.number_of_nodes('track')),\n",
    "    batch_size=32,\n",
    "    collate_fn=collator.collate_test,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch._C._TensorMeta' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1949/2471963588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_ntype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_ntype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch._C._TensorMeta' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(type(torch.arange(g.number_of_nodes(item_ntype)))[0]) \n",
    "print(type(torch.randint(1, g.number_of_nodes(item_ntype), (2000,))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = PinSAGEModel(g, 'track', 128, 2)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_it = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mila/r/rebecca.salganik/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mila/r/rebecca.salganik/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 40, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_118567/2299130797.py\", line 84, in collate_train\n    assign_features_to_blocks(blocks, self.g_train, self.ntype)\n  File \"/tmp/ipykernel_118567/2299130797.py\", line 63, in assign_features_to_blocks\n    data[col] = g.nodes[ntype].data[col][induced_nodes]\nIndexError: index 2070472 is out of bounds for dimension 0 with size 2032977\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118567/2119244004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpos_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Copy to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nsv3/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mila/r/rebecca.salganik/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mila/r/rebecca.salganik/.conda/envs/nsv3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 40, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_118567/2299130797.py\", line 84, in collate_train\n    assign_features_to_blocks(blocks, self.g_train, self.ntype)\n  File \"/tmp/ipykernel_118567/2299130797.py\", line 63, in assign_features_to_blocks\n    data[col] = g.nodes[ntype].data[col][induced_nodes]\nIndexError: index 2070472 is out of bounds for dimension 0 with size 2032977\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "batches_per_epoch = 10 \n",
    "k = 2\n",
    "batch_size = 32 \n",
    "losses = []\n",
    "\n",
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(num_epochs):\n",
    "    model.train()\n",
    "    print (\"EPOCH: {}\".format(epoch_id))\n",
    "    for batch_id in tqdm.trange(batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "\n",
    "        loss, auc = model(pos_graph, neg_graph, blocks)\n",
    "        loss = loss.mean() \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if epoch_id % 10 == 0:\n",
    "        print(loss, auc)\n",
    "        losses.append([loss.item(),auc])\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"****EVALUATION START****\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "#         item_batches = #torch.arange(g.number_of_nodes(item_ntype)).split(batch_size)\n",
    "        \n",
    "        h_item_batches = []\n",
    "        for blocks in tqdm.tqdm(dataloader_test):\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            h_item_batches.append(model.get_repr(blocks))\n",
    "        h_item = torch.cat(h_item_batches, 0)\n",
    "        print(\"****RECOMMEND****\")\n",
    "        print(evaluate_nn(dataset, h_item, k, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsv3",
   "language": "python",
   "name": "nsv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
